# Data cleaning

This part provides an introduction to data cleaning. When data is really messy, data cleaning can be a challenging task. We provide some approaches that however, will not include or solve all issues during data cleaning.

```{r}
#| label: read-data-clean
#| echo: false
#| warning: false

exDat <- readRDS("exampleDat.RDS")

```

```{css, echo=FALSE}
.vScrollbar {
  max-height: 250px;
  overflow-y: scroll;
}
```

## Why data cleaning?


The **validity ** of results of statistical data analyses depends on the **quality of the data** which in turn depends on several **factors** such as:

-   sampling (number of cases, representativeness, etc.)    
-   research design    
-   quality of the survey instrument    
-   operationalization
-   ...
-   **data cleaning & data preparation**


## What is data cleaning?

Data cleaning (or data cleansing) is the process of identifying and correcting errors, inconsistencies, and inaccuracies in the data. It is **always the first step** after data entry or data transfer from external sources.

The steps of data cleaning vary depending on the type and quality of data. The following steps can be seen as a general outline:

0.  Understand the structure of the data (set) &rarr; Codebook should help
1.  [Remove duplicate observations](#rem-duplicate)      
2.  [Handling missing data](#handle-missing)  
3.  [Plausibility checks](#plaus-checks) (aka data validation)


## How to clean data?

Depending on how much data cleaning is needed, it is recommended to clean data in a sequential way (We like it neat!). This means after we succeed with one step of data cleaning, we save the respective `R`- or `Quarto` script and the data set with an appropriate description (see @fig-seq-way-data). 


![Example of sequential way of working with data](images/seq-way-data.PNG){#fig-seq-way-data}



The sequential procedure is also exemplified in the next section [Remove duplicate observations](#rem-duplicate).

::: {.callout-important} 

When cleaning a data set, you should **NEVER(!)** replace the raw data with processed data (i.e., overwrite the raw data file). The manipulated data should be assigned to a new object and thereafter **MUST BE** saved as a new file!

The raw data should also be a read-only file (i.e., right click on the `file > Properties > Attributes > Read-only`).

:::

Because it is also crucial to document the data cleaning process (i.e., to reconstruct all steps), we also recommend to use `Quarto` documents because of the increased readability.


## Remove duplicate observations {#rem-duplicate}

Duplicate observations are identified via the `id` and `variable` values (i.e., response patterns). In @tbl-dup-obs there are 3 different scenarios depicted that are problematic. 

+----+-----------------------------------------+--------+-----------------------------------+
|    | Scenario                                |        | Action                            |
+====+=========================================+========+===================================+
| 1. | Same `id` variable value, and same      | &rarr; | delete one observation            |
|    | variable values (response pattern)      |        |                                   |
+----+-----------------------------------------+--------+-----------------------------------+
| 2. | Same `id` variable value, but different | &rarr; | check data (i.e., questionnaires) |
|    | variable values (response pattern)      |        |                                   |
+----+-----------------------------------------+--------+-----------------------------------+
| 3. | Different `id` variable values, but     | &rarr; | complicated, it is possible, but  |
|    | same variable values (response pattern) |        | check data (i.e., questionnaires, |
|    |                                         |        | especially open fields)           |
+----+-----------------------------------------+--------+-----------------------------------+

: Overview of duplicate observations scenarios {#tbl-dup-obs}

::: {.callout-tip collapse="true" appearance="simple" title="Tip: Think carefully about the identification variables!"} 

Identification (ID) variables are eminent when working on projects that contain several sources (e.g., different questionnaires), or span across multiple years (e.g., longitudinal studies) and must be **planned before the data collection**.

ID variables should be ...

- uniquely identifying &rarr; no duplicates

- fully identifying &rarr; all observations have an ID variable value

- constant throughout the duration of projects &rarr; observations do not have different IDs in a other datasets 

- anonymous 

For more see here: <https://dimewiki.worldbank.org/ID_Variable_Properties>


:::


The procedure is as follows:

0. Import data &rarr; we use the data set from below: `exDatID`

1. Find duplicate observations

2. Exclude real (!) duplicates (but consult your supervisors!)

3. Save data set to a new file 

::: {#gen-dup-obs .callout-warning collapse="false" appearance="simple" title="Generate some duplicate observations."}

**!! Do not do this with real data !! !! Do not do this with real data !! !! Do not do this with real data !!**

The example data set `exDat` does not contain any duplicate observations. Hence, we have to create them. This is done with the `rbind` function (i.e., adding the first five rows `[1:5]` of the `exDat` data set to the `exDat` data set).

```{r}
#| label: add-dup-obs
#| code-fold: false
#| code-line-numbers: true


exDatID <- tibble::as_tibble( rbind(exDat,exDat[1:5,]) )
exDatID[c(751, 752),"id"] <- c(751, 752)
exDatID[6,"id"] <- 7
#exDatID$comment <- c("Hi", "Hello", "apple", "avocado", "bye",
  #                   rep("no", 745),
 #                    "Hi", "Hello", "apple", "avocado", "bye")

nrow(exDatID)
```

**!! Do not do this with real data !! !! Do not do this with real data !! !! Do not do this with real data !!**

For the exercise [below](#ex-rem-dup), write the `exDatID` data set to your project folder.

```{r}
#| label: write-exDatID
#| eval: false

getwd()
write.csv2(x = exDatID,
           file = "exDatID.csv",
           row.names = FALSE)
```


:::



How to find and remove duplicate observations via the `id` and `variable` values? In the following there is a base `R` and a `dplyr` [@R-dplyr] solution shown. Both approaches use the `duplicated()` function which requires a `vector`, a `data frame` or an `array` as input `x`. The output is logical vector (`TRUE`/`FALSE`) of the same length as `x`.

::: {#ex-rem-dup .callout-caution collapse="true" title="Exercise: Remove the duplicate observations from the exDatId data set!"}

0. Open a new `R`- or `Quarto`-script (i.e., `File > New File`) and provide an meaningful name.

1. Import the `exDatID.csv` data set (which was generated [above](#gen-dup-obs)) 

```{r}
#| label: read-exDatID
#| eval: false
#| code-fold: false
exDatID <- read.csv2(file = "exDatID.csv",
          header = TRUE)
```


2. Go through the following steps. Choose either between the base `R` or the `dplyr` approach.

3. Save the cleaned data set (i.e., without the duplicates) with a meaningful name that also matches the name of the script.

```{r}
#| label: write-exDatID-clean
#| eval: false
#| code-fold: false
getwd()
write.csv2(x = exDatCleanID,
           file = "exDat-cleaned-ID.csv",
           row.names = FALSE)
```

:::


::: {.panel-tabset}
### Base R

1. Identify the duplicate `id` values: Use the `ifelse` function to transform the returned logical vector of the `duplicated` function to a `character` variable and then it as a new variable^[How to add a new variable is explained in more detail in the section on [Data transformation](data-transform.qmd#build-var).] (here: `dupID`) to the data set. 
  
```{r}
#| label: show-dup-1
#| code-fold: false
#| code-line-numbers: false
#| 
exDatID$dupID <- ifelse ( duplicated(exDatID$id), "dup"," ")
#table(exDatID$dupID)
```

2. Identify with the `which` function which `id` values are duplicated.

```{r}
#| label: show-dup-2
#| code-fold: false
#| code-line-numbers: false
#| 
whichID <- exDatID[which(exDatID$dupID == "dup"), "id"]
whichID
```


3. Select only the the rows with duplicated `id` values (`unlist` the `tbl_df` object: `whichID` is necessary).

```{r}
#| label: show-dup-3
#| code-fold: false
#| code-line-numbers: false
#| 
dupIDs <- exDatID[exDatID$id %in% unlist(whichID),]
```

4. Show the (ordered) data set.

```{r}
#| label: show-dup-4
#| code-fold: false
#| code-line-numbers: false
#| 
dupIDs[order(dupIDs$id),]
```


What do we see? 4 duplicated `id` values: 3, 4, 5, 7 were identified, but only 3, 4, and 5 are real duplicate observations. In a real data set, this would be probably a input error during data entry. Hence, it would be necessary to check the questionnaires. We know that it was a mistake, because we created the error (see above). Thus, we can reverse it without further ado. But be careful that you select the right observation.

```{r}
#| label: exDatID-fix-id
#| code-fold: show
#| code-line-numbers: false
exDatID[exDatID$id == 7 & exDatID$dupID != "dup","id"] <- 6
exDatID[exDatID$id == 7,"dupID"] <- " "
```


The observations with the `id` values 3, 4, and 5 can be deleted. Therefore, we can use the `subset` function that requires the input `x` which can be `matrices`, `data frames` or `vectors` (including `lists`). In the `subset`argument you can state logical expression indicating elements or rows to keep (here: `!= dup`).

```{r}
#| label: demo-subset-dup
#| code-fold: false
exDatID <- subset(x = exDatID,
                  subset = dupID != "dup")
nrow(exDatID)
```

Note that there are still 2 duplicate observations left. Nevertheless, we can delete the variable `dupID` and clean our environment with the `rm` function:

```{r}
#| label: del-dupID
#| code-fold: false
exDatID$dupID <- NULL
rm(whichID, dupIDs)
```

What to do with the other 2 duplicate observation? We need to repeat the procedure. This time we do not examine the `id` values (i.e., actually we exclude this variable), but the response pattern.

```{r}

uniqueRows <- exDatID[duplicated(exDatID[, -which(names(exDatID) == "id")]),"id"]

exDatID[exDatID$id %in% unlist(uniqueRows),]

```


<!--



```{r}
#| label: show-dup-resp
#| code-fold: false
#| code-line-numbers: true


exDatID$dupResp <- ifelse(duplicated( exDatID[, -which(names(exDatID) == "id")],),
                          "dup",
                          " ")
#table(exDatID$dupResp)
whichresp <- exDatID[which(exDatID$dupResp == "dup"), "id"]
dupResps <- exDatID[exDatID$id %in% unlist(whichresp),]
dupResps[order(dupResps$id),]
```


-->



### dplyr package

Filtering can be done using the `dplyr::filter()` function in combination with `duplicated()`.

```{r}
exDatID |> 
  dplyr::mutate(dupID = ifelse(duplicated(id), "dup", " ")) |>
  dplyr::filter(dupID == "dup")
    
  
```

However, `dplyr` provides a function `dplyr::distinct()` which can be used to keep only unique rows.

```{r}
#| class-output: vScrollbar
dplyr::distinct(exDatID)
```

:::





## Handling missing data {#handle-missing}

Missing data is a common problem in most behavioral science research [@Enders2010; @Enders2023;@Schafer2002]. Especially, in questionnaire surveys it is unavoidable that values are missing because a field of the questionnaire was not filled in or a person dropped out of the study. Therefore, it is essential to examine the data with regard to missing data. By missing values, we refer to cells that could have a value, but whose value is not available (see section on [Data Types](intro-r-rstudio.qmd#data-types) in the [Introduction to R & RStudio](intro-r-rstudio.qmd) part).

The procedure encompasses ~~3~~ 2 steps:

1.  [Define missing values](#def-miss) 
2.  [Examine missing values](#examine-miss)  
3.  ([Omit missing values](#omit-miss) &rarr; usually **not** recommended)


### Define missing values {#def-miss}

In other statistical software programs (e.g., SPSS) or in study planning, missing values are often defined as numeric values (e.g., `-9`, `-88` or `-99`)^[Note that this is also recommended when designing a codebook, see here: <https://datawizkb.leibniz-psychology.org>] or as empty characters/strings (i.e., `""` or `" "`). In `R`, missing values are represented by the symbol `NA` (not available). This means, the first step is to ensure that all missing values are declared as `NA` in the data set.

::: {.callout-warning collapse="false" appearance="simple" title="Assign -99 values"}

**!! Do not do this with real data !! !! Do not do this with real data !! !! Do not do this with real data !!**

The example data set `exDat` does also not contain any missing values that are not `NA` values. Hence, we have to create them. With the following code, we assign the values `-99` to the first 5 rows of the variable `age` and `-88` to the first 4 rows of the variables `sex` and `edu` (and store it in a new data set `exDatMis`).

```{r}
#| label: add-99
#| code-fold: false
#| code-line-numbers: false
exDatMis <- exDat
exDatMis[1:5, "age"] <- -99
exDatMis[1:4,c("sex", "edu")] <- -88
head(exDatMis, 6)
```


**!! Do not do this with real data !! !! Do not do this with real data !! !! Do not do this with real data !!**


:::

To demonstrate the effect, we calculate the mean of the variable `age` in the `exDat` and `exDatMis` data set:

```{r}
#| label: demo--99
#| code-fold: false
#| layout-ncol: 2

mean(exDat$age, na.rm = T)
mean(exDatMis$age, na.rm = T)
```

To set the numeric values of `-99` or `-88` to `NA`, we may use one of the following two approaches:

::: {.panel-tabset}
### Base R

For a single variable:

```{r}
#| label: declare-miss
#| code-fold: false
exDatMis$age[exDatMis$age == -99] <- NA
```

For multiple variables:

```{r}
#| label: declare-miss-mult-var
#| code-fold: false

colToNa <- c("sex", "edu")
exDatMis[colToNa][exDatMis[colToNa] == -88] <- NA
```

For the whole data set:

```{r}
#| label: declare-miss-whole
#| code-fold: false
#| eval: false
exDatMis[exDatMis == -99] <- NA
```

### dplyr package

The `na_if()` function from the `dplyr` package [@R-dplyr] is designed to to convert a (specific) values to `NA`.

For a single variable:

```{r}
#| label: declare-miss-dply
#| code-fold: false
#| eval: false

exDatMis$age <- dplyr::na_if(exDatMis$age, -99)
```

For multiple variables:

```{r}
#| label: declare-miss-mult-var-dplyr
#| code-fold: false
#| eval: false
colToNa <- c("sex", "edu")
exDatMis <- exDatMis |>
  dplyr::mutate(dplyr::across(colToNa, na_if, y = -88)) 
```


For the whole data set:

```{r}
#| label: declare-miss-whole-dplyr
#| code-fold: false
#| eval: false


exDatMis <- exDatMis |>
  dplyr::mutate(dplyr::across(colnames(exDatMis), na_if, y = -88))
```

But note that `na_if()` is meant for use with vectors rather than entire data frames.

:::




### Examine missing values {#examine-miss}

The `is.na` function indicates which elements are missing. It requires an input `x` that can be e.g., a `vector`, `data.frame` or `list`. The values that are returned depend on the input. For example, when you pass a `vector` to the function, it returns a `logical` `vector` of the same length as its argument `x`, containing `TRUE` for those elements marked `NA` or, for `numeric` or complex `vectors`, `NaN`, and `FALSE` otherwise.

```{r}
#| label: demo-isna
#| code-fold: false
is.na(
  c(1, NA, "hello", NaN, " ")
  )


```
In the following, we show how the `is.na` function may be applied to variables within a data set. To count the missing values (or to be more specific the returned `TRUE` values), we use the `sum` and `colSums` functions. 

::: {.callout-tip collapse="true" appearance="simple" title="Tip: summary() of a data.frame"}
The `summary()` function returns beside some descriptive statistics (e.g., minimum, maximum, mean, ...), also the amount of `NA`s of each variable of the given data set.

```{r}
#| label: demo-summary-miss
#| code-fold: false
summary(exDat)
```

:::


::: {.panel-tabset}
### Base R

For a single variable:

```{r}
#| label: demo-count-miss-single
#| code-fold: false

sum(is.na(exDat$msc2))

```

For multiple variables:

```{r}
#| label: demo-count-miss-mult
#| code-fold: false

colSums(is.na(exDat[,c("msc2", "sex", "edu")]))


```

For the whole data set:

```{r}
#| label: demo-count-miss-whole
#| code-fold: false

colSums(is.na(exDat))


```

### dplyr package

For a single variable:

```{r}
#| label: demo-count-miss-single-dplyr
#| code-fold: false
exDat |>
  dplyr::select(msc2) |>
  is.na() |>
  sum()
```


For multiple variables:

```{r}
#| label: demo-count-miss-mult-dplyr
#| code-fold: false

exDat |>
  dplyr::select(c("msc2", "sex", "edu")) |>
  is.na() |>
  colSums()
  


```

For the whole data set:

```{r}
#| label: demo-count-miss-whole-dplyr
#| code-fold: false

exDat |>
  is.na() |>
  colSums()

```

:::


::: {.callout-caution collapse="true"}
## Exercise: Count the missing values!

:::



### Omit missing values {#omit-miss}

Although omitting or deleting missing values is a common practice, this is not recommended to deal with missing data and should--in most scenarios--be avoided altogether [@Schafer2002]. Two "state-of-the-art" missing data methods are maximum likelihood estimation [e.g., implemented in the `lavaan` package, see @R-lavaan] and multiple imputation [e.g., implemented in the `mice` package, see @R-mice; for good introductions to this topic, see @Enders2010].

Nevertheless, if you would like to omit missing values anyway, there are several functions to do this in `R`. Also, it is important to note that ignoring or omitting missing values are often the default options in `R`.

For example, by negating the `is.na` function (i.e., `!is.na`), those rows that do not contain `NA`s on a variable or data set are kept.

::: {.panel-tabset}

### Base R

For a single variable:

```{r}
#| label: omit-na-single
#| code-fold: false
#| class-output: vScrollbar

exDat[!is.na(exDat$age), ]

```


For the whole data set:

The `na.omit` function returns the object with incomplete cases removed.

```{r}
#| label: omit-na-data
#| code-fold: false
#| class-output: vScrollbar

exDatnoNA <- na.omit(exDat)
exDatnoNA
```

### dplyr package

To exclude observations that do have missing values, we may use the `filter` function from the `dplyr` package. 

For a single variable:

```{r}
#| label: omit-na-single-dplyr
#| code-fold: false
#| class-output: vScrollbar

exDat |>
  dplyr::filter(!is.na(age))
```

For the whole data set:

```{r}
#| label: omit-na-whole-dplyr
#| code-fold: false
#| class-output: vScrollbar

exDatnoNA <- exDat |>
  dplyr::filter() |>
  na.omit()
exDatnoNA
```

:::



## Plausibility checks {#plaus-checks}


So-called plausibility checks are performed in order to detect:

-   structural errors &rarr; e.g., the range of a lickert scale is from 1 to 4, but the value 5 occurs  
-   theoretical inconsistencies &rarr; e.g., 30 years of age in sample of primary students   
-   (statistical) outliers &rarr; see Excursus on outlier [below](#excurs-outlier)


::: {.callout-note collapse="true" appearance="simple"}
## Excursus: Outliers {#excurs-outlier}

According to [wikipedia](https://en.wikipedia.org/wiki/Outlier), outliers can be defined as a data point that differs significantly from other observations.

Outliers may occur due to different reasons:

- data entry errors
- measurement error
- "true" extreme values 

Outliers must be detected, because they may introduce bias into parameter estimates of statistical models and hence, compromise the validity of the results.

Different ways of detecting outliers:    

-   Z-score (e.g., above or below $3SD$)
-   Visualization (e.g., box plots, histograms, or scatter plot) 
-   Mahalanobis distance
- ...



:::

Plausibility checks can be performed both analytically and graphically. Graphics can be created with base `R` [@R-base], or packages such as `lattice` [@R-lattice] and `ggplot2` [@R-ggplot2] that are specifically designed for data visualization. Although base `R` graphics are useful (especially for creating simple plots), `ggplot2` more powerful and flexible for creating complex visualizations. You may load the package via: 

```{r}
#| label: load-ggplot2
#| code-fold: false
#| warning: true

library(ggplot2)
```

Note that `ggplot2` package [@R-ggplot2] is part of the `tidyverse` and it nicely works with the `dplyr` package [@R-dplyr]. For an introduction see the `tidyverse` [documentation of ggplot2](https://ggplot2.tidyverse.org/).

To perform plausibility checks analytically, the `range` function is a useful starting point to quickly examine the minimum and maximum of variables that are no characters/string variables (i.e., `!is.character`).

::: {.panel-tabset}
### Base R

1. Identify all non-character variables using the `!is.character` function within a `lapply` loop. Because `lapply` returns a list, we need to `unlist` the output to get a logical vector.

```{r}
#| label: check-range-1
#| code-line-numbers: true

nChrV <- unlist(lapply(exDatMis,
                         function(x) !is.character(x)),
                use.names = FALSE)
#class(nChrV)
```

2. Then we can apply the `range` function to the respective variable within the `exDatMis`.

::: {.callout-tip collapse="true" appearance="simple"}
## Tip: The apply function {#apply-func}

The `apply` function is designed to apply functions (e.g., `range`) over array margins (e.g., rows or columns) and needs the following arguments (copied from the function description):

- `X`: an array, including a matrix.
- `MARGIN`: a vector giving the subscripts which the function will be applied over. E.g., for a matrix 1 indicates rows, **2 indicates columns**, c(1, 2) indicates rows and columns. Where X has named dimnames, it can be a character vector selecting dimension names.
- `FUN`: the function to be applied: see ‘Details’.

:::

```{r}
#| label: check-range-2
#| code-line-numbers: true
apply(exDatMis[,nChrV],
      2,
      FUN = range, na.rm=TRUE)

```
### dplyr package

In the `dplyr` package we first use the `select` function (i.e., to select the variables) in combination with `where` function to identify all non-character variables by stating `!is.character`. Then applying the `range` function on very variable in the data set.

```{r}
#| label: check-range-dplyr

exDatMis |>
  dplyr::select(
    dplyr::where(~!is.character(.))
    ) |>
  dplyr::summarise_all("range", na.rm = TRUE)

```

:::


::: {.callout-caution collapse="true"}
## Exercise: Set the value 30 of the age variable to NA.

1. Identify the observation with `age == 30`, using the `which` function.

```{r}
#| label: ident-age-30
#| code-fold: false
exDat[which(exDat$age == 30),c("id", "age")]

```

2. Set the value 30 to `NA`

```{r}
#| label: make-age-30-NA
#| code-fold: false
exDat[which(exDat$age == 30),"age"] <- NA

```

:::



### Categorical variables

#### Frequencies/Counts {#cat-freq-count}

To do further plausibility checks on categorical variables, we may examine counts (at each combination) of factor levels (e.g., examine extreme responses in one category).

::: {.panel-tabset}
##### Base R

The `table` function builds a contingency table of the counts at each combination of factor levels. 

::: {.callout-note collapse="true" appearance="simple"}

Note that by default, `NA`s are not included in the `table` function. To include `NA`s, we need to change the `useNA` argument to `"ifany"` (only if the count is positive) or `"always"` (even for zero counts).

:::

```{r}
#| label: clean-demo-table
#| code-fold: false
table(exDatMis$edu, useNA = "always")
```

##### dplyr package

The `dplyr` approach is using the `count` function that counts the unique values of one or more variables.

```{r}
#| label: clean-demo-count
#| code-fold: false
exDatMis |>
  dplyr::count(edu) 

```

:::

#### Graphical inspection using bar charts 

Bar charts are one way of graphically checking the plausibility of discrete variables. 

::: {.panel-tabset}

##### Base R

```{r}
#| label: fig-bar-chart-base
#| fig-cap: "Frequencies of the variable education in the example data set (barplot)"
#| eval: true


barplot(
  height = table(exDat$edu, useNA = "always"),
  names.arg = c(0:4, "NA"),
  #main = "Education in example data set (barplot)", # plot title
  xlab = "edu", # x-axis label
  ylab = "Count",  # y-axis label
  ylim = c(0, 300) # y-axis range
  )
```




##### ggplot2 package

```{r}
#| label: fig-bar-chart-ggplot
#| fig-cap: "Frequencies of the variable education in the example data set (ggplot2)"
#| eval: true

exDat |>
  dplyr::count(edu) |>
  ggplot(aes(x = edu, y = n)) +
  geom_bar(stat = "identity", color = "black", fill = "lightgrey") +
  geom_text(aes(label=paste("n = ", n)), vjust=1.6, color="black", size=3.5)+
  labs(x = "edu", y = "Count") +
  theme_classic()
```

:::

### Continuous variables

Continuous variables are checked by examining the distribution of the variables. It is reasonable to start checking the range (i.e., minimum & maximum) of the variable (using the `range` function, see e.g., [above](#plaus-checks)). 

#### Distribution parameters/measures

Furthermore, the following Distribution parameters/measures might be informative:

- location: mean, median &rarr; `base::mean`, `stats::median`
- dispersion: standard deviation, variance &rarr; `stats::sd`, `stats::var`
- shape (asymmetry): skewness &rarr; `moments::skewness`
- shape (tailedness): kurtosis &rarr; `moments::kurtosis`

In the following, we show how to examine the skewness and kurtosis. The calculation of the other parameters is introduced in the section on [Descriptive statistics and item analysis](#item-analy-descr).

:::: {.columns}

::: {.column width="47.5%"}

```{r}
#| label: demo-skewness
#| code-fold: false
moments::skewness(exDat$age,
                  na.rm = T)
```


:::

::: {.column width="5%"}
<!-- empty column to create gap -->
:::

::: {.column width="47.5%"}

```{r}
#| label: demo-kurtosis
#| code-fold: false

moments::kurtosis(exDat$age,
                  na.rm = T)
```
:::

::::

What are the cutoffs for skewness and kurtosis? It depends on the specific context...

- Skewness: 0 (perfect symmetry) &rarr; cutoff: 1-2
  - $> 0$: right-skewed distributions 
  - $< 0$: left-skewed distributions
- Kurtosis: 3 (perfect normal distribution) &rarr; cutoff: 6-7
  - $> 3$: more peaked distribution
  - $< 3$: a flatter distribution 

(Reference needed)

#### Graphical inspection using histogram

Histograms are graphical representations of frequency (or relative frequency) distributions of variables and are used to visualize the shape of a distribution. 


::: {.panel-tabset}

##### Base R


```{r}
#| label: fig-demo-histogram
#| fig-cap: "Frequency distribution of age in example data set"
#| eval: true

hist(exDat$age,
     main = "Age in example data set",
     breaks = 20,
     xlim = c(min(round(exDat$age), na.rm = T) - 1,
              max(round(exDat$age), na.rm = T) + 1),
     ylim = c(0, 100)
     )
```

##### ggplot2 package

```{r}
#| label: fig-demo-histogram-ggplot2
#| fig-cap: "Frequency distribution of age in example data set (ggplot2)"
#| eval: true
#| warning: false

ggplot(data = exDat, #na.omit
       aes(x = age)) +
  geom_histogram(bins = 100,
                 binwidth = .5,
                 color = "black",
                 fill = "white") +
  geom_vline(data = exDat, aes(xintercept = mean(age, na.rm = T)),
             linetype="dashed",
             color="red") +
  scale_x_continuous(limits = c(
    min(exDat$age,
        na.rm = T)-1,
    max(exDat$age,
        na.rm = T)+1)) +
  theme_minimal()
```

:::



#### Graphical inspection using boxplots

Boxplots allow visualization of the most important robust measures of location and dispersion.

::: {.panel-tabset}

##### Base R

```{r}
#| label: fig-range-of-items-boxplots
#| fig-cap: "Boxplot of variable age in the example data set"
#| eval: true


boxplot(exDat$age,
        #main = "plot title",  
        xlab = "Variables", # x-axis label
        ylab = "Range" # y-axis label
        )
```


##### ggplot2 package

```{r}
#| label: fig-range-of-items-boxplots-ggplot
#| fig-cap: "Boxplot of variable age in the example data set (ggplot2)"
#| eval: true
#| warning: false

ggplot(data = exDat, #na.omit
       aes(y = age, x = factor(0))) +
  geom_boxplot(width=.5, outlier.color = "red") +
  #geom_jitter(width = .25, color = "grey", alpha = .2) +
  scale_x_discrete(breaks = NULL) +
  xlab(NULL) +
  theme_bw()
```



:::


