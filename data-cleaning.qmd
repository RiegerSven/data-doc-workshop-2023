# Import data & data cleaning

```{r}
#| label: data-cleaning-prep
#| echo: false
#| warning: false

exDat <- readRDS("exampleDat.RDS")

library(dplyr)
```

This section provides an introduction to importing data sets and data cleaning in general. When data is really messy, data cleaning can be a challenging task. We provide some approaches that however, will not include or solve all issues during data cleaning. 

```{css, echo=FALSE}
.vScrollbar {
  max-height: 250px;
  overflow-y: scroll;
}
```

## Import (or export) data sets

In `R` data sets are usually stored as `data.frame` objects (see also the section [Data Structures](intro-r-rstudio.qmd#data-structure)). There are other object types for data sets such as `tibble` from the `tibble` package [@R-tibble] or `data.table` from the `data.table` package [@R-data.table] which are more efficient, especially when it comes to (very) large data sets (see e.g., [Chapter 10](https://r4ds.had.co.nz/tibbles.html) of [R for Data Science](https://r4ds.had.co.nz/) and the [vignette](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) of the `data.table` package).

### Data frames

Data sets can come in many different file formats. For example, `.txt`, `.csv` files, or `.sav` ([SPSS](https://www.ibm.com/spss)), `dta` ([Stata](https://www.stata.com/)), `.sas7bdat` ([SAS](https://www.sas.com/en_us/home.html)) files.

To import e.g., `.csv` or `.txt` files, `R` offers a couple of functions that deal with these formats:

- `read.table`: Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file.
    - `read.csv` or `read.csv2`^[see also here: <https://stackoverflow.com/questions/22970091/difference-between-read-csv-and-read-csv2-in-r>]
    - `read.delim` or `read.delim2` 

- `readLines`: Read some or all text lines from a connection.

- ...

When it comes to other (software specific) formats, you need additional packages e.g., `foreign` [@R-foreign] or `haven` [@R-haven]. In the following, we first export the `exDat` data set to a `.sav` file using the `write.foreign` function from the `foreign` package.

To import the SPSS data sets (`.sav` file), we may use the `read.spss` function from the `foreign` package [@R-foreign]. There are a couple of default arguments which you (likely) want to change:

- `use.value.labels`: logical: convert variables with value labels into R factors with those levels? (default is `TRUE` switch to `FALSE`)
- `to.data.frame`logical: return a data frame? (default is `FALSE` switch to `TRUE`)
- `use.missings`: logical: should information on user-defined missing values be used to set the corresponding values to NA?

```{r}
#| label: demo-read-spss
#| eval: false
#| code-fold: false

exDatImpforeign <- foreign::read.spss("file-to-path-with-.sav-file",
                                      use.value.labels = FALSE,
                                      to.data.frame = TRUE,
                                      use.missings = TRUE)

```

::: {.callout-note collapse="true" appearance="simple"}
## Exporting dataframes

To export data to a file, you can use the `write` function(s):

- `write.table`
- `write.csv` or `write.csv2`
- ...

Again, when it comes to software-specific file formats you need additional packages such as `foreign` [@R-foreign] or `haven` [@R-haven]:

- `haven::write_sav`
- `foreign::write.foreign`
- ...


In the following, the use of the `write.foreign` function is displayed:

```{r}
#| label: demo-write-foreign
#| eval: false
#| code-fold: false 


#getwd()
codefile <- tempfile()

foreign::write.foreign(exDat,
                       "exDat.sav",
                       codefile = codefile,
                       package = c("SPSS"))

unlink(codefile)
```

Note that there are a couple of further steps necessary to get a working `.sav` file. We recommend using the `haven` [@R-haven] package (see [below]()), at least when it comes to exporting data sets.

::: 

### Tibbles

The tibble package is part of the `tidyverse`^[for more see the section on [Base R ~~vs.~~ & tidyverse](intro-r-rstudio.qmd#baseR-tidyverse).] [@R-tidyverse].

First, check the class of the example data set `exDat`.

```{r}
#| label: class-exDat
#| code-fold: false
class(exDat)
```
The `as_tibble` function from the `tibble` package [@R-tibble] turns an existing object (a data frame or matrix) into `tibble` which is a data frame with `class` `tbl_df` (see `?as_tibble`).


```{r}
#| label: demo-as-tibble
#| code-fold: false
exDatTib <- tibble::as_tibble(exDat)
class(exDatTib)
```

The `write_sav` function from the `haven` package [@R-haven] can also be used to export data sets. Before exporting the data, you may want to check your working directory with the `getwd()` function.

```{r}
#| label: demo-write-sav
#| eval: false
#| code-fold: false 


#getwd()
haven::write_sav(exDat, "exDat.sav")

```

To import the data set, we use the `read_sav` function from the `haven` package.

```{r}
#| label: demo-read-sav
#| eval: false
#| code-fold: false

exDatImpTest <- haven::read_sav("exDat.sav")

```

Note that the `read_sav` returns a `tibble`, `data frame` variant (with nice defaults).

For more see <https://tibble.tidyverse.org/> and [Chapter 10](https://r4ds.had.co.nz/tibbles.html) of [R for Data Science](https://r4ds.had.co.nz/).

## Why data cleaning?


The **validity ** of results of statistical data analyses depends on the **quality of the data**. The quality of the data depends on several factors such as:

-   sampling (number of cases, representativeness, etc.)    
-   research design    
-   quality of the survey instrument    
-   operationalization
-   ...
-   **data cleaning & data preparation**


## What is data cleaning?

Data cleaning (or data cleansing) is the process of identifying and correcting errors, inconsistencies, and inaccuracies in the data. It is **always the first step** after data entry or data transfer from external sources.

The steps of data cleaning vary depending on the type and the quality of data. The following steps can be seen as a general outline:

0.  Understand the structure of the data (set) &rarr; Codebook should help
1.  [Remove duplicate observations](#rem-duplicate)      
2.  [Handling missing data](#handle-missing)  
3.  [Plausibility checks](#plaus-checks) (aka data validation)


## How to clean data?

Depending on how much data cleaning is needed, it is recommended to clean data in a sequential way (We like it neat!). This means after we succeed with one step of data cleaning, we save the respective `R`- or `Quarto` script and the data set with an appropriate description. The sequential procedure is exemplified in the [Remove duplicate observations](#rem-duplicate) section.

::: {.callout-important} 

When cleaning a data set, you should **NEVER(!)** replace the raw data with processed data (i.e., overwrite the raw data file). The manipulated data should be assigned to a new object and thereafter **MUST BE** saved as a new file (see also below)!

The file of the raw data should also be a read-only file (i.e., right click on the `file > Properties > Attributes > Read-only`).

:::

 
Because it is also crucial to document the data cleaning process (i.e., to reconstruct all steps), we also recommend to use `Quarto` documents because of the increased readability.




## Remove duplicate observations {#rem-duplicate}

Duplicate observations are identified via the `id` and `variable` values (i.e., response patterns). In @tbl-dup-obs there are 3 different scenarios depicted that are problematic. 

+----+-----------------------------------------+--------+-----------------------------------+
|    | Scenario                                |        | Action                            |
+====+=========================================+========+===================================+
| 1. | Same `id` variable value, and same      | &rarr; | delete one observation            |
|    | variable values (response pattern)      |        |                                   |
+----+-----------------------------------------+--------+-----------------------------------+
| 2. | Same `id` variable value, but different | &rarr; | check data (i.e., questionnaires) |
|    | variable values (response pattern)      |        |                                   |
+----+-----------------------------------------+--------+-----------------------------------+
| 3. | Different `id` variable values, but     | &rarr; | complicated, it is possible, but  |
|    | same variable values (response pattern) |        | check data (i.e., questionnaires) |
+----+-----------------------------------------+--------+-----------------------------------+

: Overview of duplicate observations scenarios {#tbl-dup-obs}

::: {.callout-tip collapse="true" appearance="simple" title="Think carefully about the identification variables!"} 

Identification (ID) variables are eminent when working on projects that contain several sources (e.g., different questionnaires), or span across multiple years (e.g., longitudinal studies) and must be **planned before the data collection**.

ID variables should be ...

- uniquely identifying &rarr; no duplicates

- fully identifying &rarr; all observations have an ID variable value

- constant throughout the duration of projects &rarr; observations do not have different IDs in a other datasets 

- anonymous 

For more see here: <https://dimewiki.worldbank.org/ID_Variable_Properties>


:::


The procedure is as follows:

1.  Find duplicate observations    
2.  Exclude real (!) duplicates (but consult your supervisors!)

::: {.callout-warning collapse="false" appearance="simple" title="Generate some duplicate observations."}

The example data set `exDat` does not contain any duplicate observations. Hence, we have to create them. This is done with the `rbind` function (i.e., adding the first five rows `[1:5]` of the `exDat` data set to the `exDat` data set).

```{r}
#| label: add-dup-obs
#| code-fold: false
#| code-line-numbers: true


exDatID <- rbind(exDat,exDat[1:5,])
exDatID[c(751, 752),"id"] <- c(751, 752)
exDatID[6,"id"] <- 7
nrow(exDatID)
```

::: {.callout-imporant}
**!! Do not do this with real data !!**
:::

:::



How to find and remove duplicate observations via the `id` and `variable` values? In the following there are a base `R` and a `dplyr` [@R-dplyr] solution. Both approaches use the `duplicated()` function which requires a `vector`, a `data frame` or an `array` as input `x`. The output is logical vector (`TRUE`/`FALSE`) of the same length as `x`.


::: {.panel-tabset}
### Base R

The `duplicated()` function checks for each value whether it is duplicated.  In combination with the `which()` function, checking which indices are `TRUE`, we can extract the duplicates.

1. Check for duplicate `id` values. The procedure contains 4 steps.

```{r}
#| label: show-dup
#| code-line-numbers: true
#| 
exDatID$dupID <- ifelse ( duplicated(exDatID$id), "dup"," ")

whichID <- exDatID[which(exDatID$dupID == "dup"), "id"]

dupIDs <- exDatID[exDatID$id %in% whichID,]
dupIDs[order(dupIDs$id),]
```


```{r}



table(exDatID$dupID)

table(ifelse ( duplicated(exDatID[,!names(exDatID) %in% c("id")]), 1, 0))

which(duplicated(exDatID[,"id"]))

#exDatID[which(duplicated(exDatID$id)), ]

exDatID$dupID <- NULL
```

By negating the function (`!duplicated()`), duplicates can be excluded.

```{r}
#| class-output: vScrollbar
exDatID[which(!duplicated(exDatID$id)), ]
```

### dplyr package

Filtering can be done using the `dplyr::filter()` function in combination with `duplicated()`.

```{r}
exDatID |> 
  dplyr::filter(duplicated(id))
```

However, `dplyr` provides a function `dplyr::distinct()` which can be used to keep only unique rows.

```{r}
#| class-output: vScrollbar
dplyr::distinct(exDatID)
```
:::

::: {.callout-caution collapse="true"}
## Exercise: Remove duplicates!

Remove all duplicates contained in `exampleDat_ID.RDS` and assign the dataFrame to an object named `exDat_uniqueID`.

:::



## Handling missing data {#handle-missing}

Missing data is a common problem in most behavioral science research [@Enders2010; @Enders2023;@Schafer2002]. Especially, in questionnaire surveys it is unavoidable that values are missing because, for example, a field of the questionnaire was not filled in. Therefore, it is essential to examine the data with regard to missing data. By missing values, we refer to cells that could have a value, but whose value is not available (see section on [Data Types](intro-r-rstudio.qmd#data-types) in the [Introduction to R & RStudio](intro-r-rstudio.qmd) part).

The procedure encompasses ~~3~~ 2 steps:

1.  [Define missing values](#def-miss) 
2.  [Check for missing values]    
3.  ([Omit missing values](#omit-miss) &rarr; usually **not** recommended)


### Define missing values {#def-miss}

In other statistical software programs (e.g., SPSS) or in study planning, missing values are often defined as numeric values (e.g., `-9`, `-88` or `-99`)^[Note that this is also recommended when designing a codebook, see here: <https://datawizkb.leibniz-psychology.org>] or as empty characters/strings (i.e., `""` or `" "`). In `R`, missing values are represented by the symbol `NA` (not available). This means we have to ensure that all missing values are declared as `NA` in the data set.

::: {.callout-warning collapse="false" appearance="simple" title="Assign -99 values"}

The example data set `exDat` does also not contain any missing values that are not `NA` values. Hence, we have to create them. With the following code, we assign `-99` to the first 5 rows of the variable `age` to a new data set `exDatMis`.

```{r}
#| label: add-99
#| code-fold: false
#| code-line-numbers: false
exDatMis <- exDat
exDatMis[1:5, "age"] <- -99
head(exDatMis, 6)
```

::: {.callout-imporant}
**!! Do not do this with real data !!**
:::

:::

To demonstrate the effect, we calculate the mean of the variable `age` in the `exDat` and `exDatMis` data set:

```{r}
#| label: demo--99
#| code-fold: false
#| layout-ncol: 2

mean(exDat$age, na.rm = T)
mean(exDatMis$age, na.rm = T)
```

To fix this issue, we may use the following two approaches:

::: {.panel-tabset}
### Base R

For a single variable

```{r}
#| label: declare-miss
#| code-fold: false
exDatMis$age[exDatMis$age == -99] <- NA
```

For the whole data set

```{r}
#| label: declare-miss-whole
#| code-fold: false
#| eval: false
exDatMis[exDatMis == -99] <- NA
```

### dplyr package

`dplyr::na_if()` can be used to convert a specific value contained in a variable to `NA`.

```{r}
#| eval: false
exDat$age <- dplyr::na_if(exDat$age, -99)
```
:::

### Check for missing values

For this part of the data cleaning process, the `dplyr` package offers no particular advantages over `R Base`.

The `summary()` function that gives an overview of the quartils, the mean and the amount of `NA`s of each variable of the given dataFrame.

```{r}
summary(exDat)
```

When considering a single variable, the sum of cells with missing values can be calculated using the function `sum()`.

```{r}
sum(is.na(exDat$msc2))
```

::: {.callout-tip collapse="true"}
## Tip for advanced users

You can apply this function to all variables of the data set:

```{r}
sapply(exDat, function(x) sum(is.na(x)))
```
:::



### Omit missing values {#omit-miss}

::: {.panel-tabset}

### Base R

By negating the `is.na()` function, those rows that do not contain `NA`s on a variable are kept.

```{r}
#| eval: false

exDat[which(!is.na(exDat$age)), ]
```

`na.omit()` returns only those rows in a dataFrame which do not contain any `NA`s.

```{r}
#| class-output: vScrollbar

na.omit(exDat)
```

### dplyr package

Like in [Identify duplicates], exclusion of missings can be done using `dplyr::filter()`. The following code filters all cases that do not have a missing value in all variables.

```{r}
#| class-output: vScrollbar

exDat |> 
  dplyr::filter(if_all(everything(),
                       ~ !is.na(.x)))
```

:::

::: {.callout-caution collapse="true"}
## Exercise: Remove missing values!

Define `-99` as `NA` across all columns and remove all rows including missing values from dataFrame `exampleDat.RDS` and assign the dataFrame to an object `exDat_noNA`.
:::

## Plausibility checks {#plaus-checks}

Plausibility checks can be performed analytically or graphically.

**Purpose:** Detecting    

-   structural errors (e.g. in coding and input)    
-   theoretical inconsistencies   
-   variables with high proportions of missing values    
-   outliers

::: {.callout-note collapse="true"}
## Excursus: Outliers

**Definition: +/- 2 or 3 standard deviations**

> "Extreme observation value that signals a qualitative element that differs from the totality."
>
> ::: {style="text-align: right;"}
> -- RÃ¶nz & Strohe, 1994
> :::

Outliers must be detected, as they can introduce bias into parameter estimates and compromise validity.

**Different ways of handling outliers:**    

-   Box plots and histograms   
-   Univariate distributions   
-   Examine extreme values   
-   Examine distributions of subgroups    
-   Multivariate scatterplots

**Recommended procedure for handling outliers:**   
(personal preference)

1.    Examine extreme values    
      1.    z-standardize variables   
          $$z = \frac{x_{m} - \bar{x}}{s_{x}}$$ {#eq-z-standardization}   
      2.    examine for values above -/+ 2 or -/+ 3 standard deviations   
2.    Coded these extreme values to `1` in a dummy variable    
      -   `1` = outlier, `0` = no outlier

:::

### Discrete variables
(= countable)    

-   After assigning the value labels and after coding the missing values   
-   Then check if non-coded numerical values exist

**Procedure:**

1.  [Frequency tables]   
2.  [Bar charts]   
3.  [Range of items]

#### Frequency tables {-}

`R` provides a function `table()` "to build a contingency table of the counts at each combination of factor levels" [@R-base]. By default, `NA`s are not included in the table. This can be changed by the argument `useNA`. It accepts a string as input with the accepted values `"ifany"` (only if the count is positive) and `"always"` (even for zero counts.

```{r}
table(exDat$edu, useNA = "always")
```

This function can be embedded in another function `proportions()` which creates a relative frequency table. As the [R Documentation on `proportions()`](https://rdrr.io/r/base/proportions.html) notes, "`prop.table()` is an earlier name, retained for back-compatibility."

Proportions are often numerics with many decimal places. Therefore, it may be useful to round the values to four decimal places or to convert them to percentages with two decimal places.

```{r}
round(proportions(table(exDat$edu)), 4)

round(proportions(table(exDat$edu)) * 100, 2)
```

#### Bar charts {-}

Bar charts are one way of graphically checking the plausibility of discrete variables. Plots can be created with `Base R` [@R-base], as shown below. However, there are also packages like `ggplot2` [@R-ggplot2] that are specifically designed for data visualization. See the [tidyverse documentation](https://ggplot2.tidyverse.org/) of `ggplot2-package` for more details.

```{r}
#| label: fig-bar-charts
#| fig-cap: "Frequencies of the education expression in the example data set"
#| eval: false


barplot(table(exDat$edu, useNA = "always"),
        names.arg = c(0:4, "NA"),
        main = "Education in example data set", # plot title
        xlab = "exDat$edu", # x-axis label
        ylab = "Count",  # y-axis label
        ylim = c(0, 300) # y-axis range
        )
```

#### Range of items {-}

::: {.panel-tabset}

### Analytical

To explicitly find out the range of a variable, the function `range()` can be used. By default, the function includes `NA`s. Use the argument `na.rm` to specify whether missings should be excluded.

```{r}
range(exDat$edu, na.rm = T)
```

This function can be applied to all numeric variables and generates a matrix.

```{r}
#| eval: false
#| 
sapply(exDat,
       function(x) if (is.numeric(x)) range(x, na.rm = T))
```

```{r}
#| label: tbl-range-of-items-all-ranges
#| code-fold: true
#| tbl-cap: Ranges of all numeric variables in example data set

exDat_ranges <- data.frame(
  sapply(exDat,
         function(x) if (is.numeric(x)) range(x, na.rm = T)),
  row.names = c("min", "max")
)

knitr::kable(exDat_ranges,
             row.names = T) |> 
  kableExtra::kable_classic("hover")
```

### Graphical

Boxplots allow visualization of the most important robust measures of location and dispersion.

![Different parts of a boxplot ([Galarnyk, 2019](https://www.kdnuggets.com/2019/11/understanding-boxplots.html))](images/parts-of-boxplot.png)

```{r}
#| label: fig-range-of-items-boxplots
#| fig-cap: "Ranges of all discrete variables in example data set"
#| eval: false


boxplot(exDat[, -5],
        main = "Ranges of all discrete variables in example data set", # plot title
        xlab = "Variables", # x-axis label
        ylab = "Range" # y-axis label
        )
```


:::

### Metric variables

(= infinite number of real values within a given interval)

-   Code missing values first!    
-   Afterwards: Consideration of the observed range of values   
    -   Question: Are there any unplausible values? (e.g. `age` = `-9`)

**Procedure:**

1.  [Descriptive statistics]    
2.  [Histogram]

#### Descriptive statistics {-}

The package `psych` [@R-psych] provides useful functions for descriptive statistical analyses.

::: {.panel-tabset}

### Base R

The `summary()` function, introduced in [chapter 4.2.1](#check-for-missibg-values), provides an overview of descriptive statistics.

```{r}
summary(exDat$age)
```

### psych package

The function `psych::describe()` contains additional descriptive statistics to those of the `Base R` function (e.g. number of valid cases, skewness, kurtosis, standard error).

```{r}
psych::describe(exDat$age)
```

:::

#### Histogram {-}

Histograms are graphical representations of frequency distributions of metric variables. Just like [bar charts](#bar-charts), these visualizations can be modified more using packages like `ggplot2` [@R-ggplot2].

```{r}
#| label: fig-histogram
#| fig-cap: "Frequency distribution of age in example data set"
#| eval: false

hist(exDat$age,
     main = "Age in example data set",
     breaks = 20,
     xlim = c(min(round(exDat$age), na.rm = T) - 1,
              max(round(exDat$age), na.rm = T) + 1),
     ylim = c(0, 100))
```

::: {.callout-tip collapse="true"}
## Plot probability density

```{r}
#| label: fig-histogram-density
#| fig-cap: "Probability density of age in example data set"
#| eval: false


# histogram with probability density
hist(exDat$age,
     main = "Age in example data set",
     probability = T)
# draw mean
abline(v = mean(exDat$age, na.rm = T),
       col = "red",
       lwd = 3)
# draw probability density line
lines(density(exDat$age, na.rm = T),
      col = "green",
      lwd = 3)
```

:::


