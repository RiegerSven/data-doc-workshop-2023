
@book{Kunter2002,
 author = {Kunter, Mareike and Sch{\"u}mer, Gundel and Artelt, Cordula and Baumert, J{\"u}rgen and Klieme, Eckhard and Neubrand, Michael and Prenzel, Manfred and Schiefele, Ulrich and Schneider, Wolfgang and Stanat, Petra and Tillmann, Klaus-J{\"u}rgen and Wei{\ss}, Manfred},
 year = {2002},
 title = {PISA 2000: Dokumentation der Erhebungsinstrumente [PISA 2000:~Documentation of instruments]},
 url = {http://hdl.handle.net/hdl:11858/00-001M-0000-0023-9987-C},
 address = {Berlin},
 urldate = {28.11.2018},
 volume = {Nr. 72},
 publisher = {{Max-Planck-Inst. f{\"u}r Bildungsforschung}},
 isbn = {3-87985-086-0},
 series = {Materialien aus der Bildungsforschung},
 file = {http://gso.gbv.de/DB=2.1/PPNSET?PPN=726009482},
 file = {http://hdl.handle.net/11858/00-001M-0000-0025-93D7-2}
}


@misc{Wissik2015,
 author = {Wissik, T. and {\v{D}}ur{\v{c}}o, M.},
 year = {2015},
 title = {Research data workflows: From research data lifecycle models to institutional solutions},
 url = {http://www.ep.liu.se/ecp/123/008/ecp15123008.pdf},
 series = {CLARIN 2015 Selected Papers, Link{\"o}ping Electronic Conference Proceedings, Annual Conference 2015, October 14--16, 2015, Wroclaw, Poland (pp. 94--107). Link{\"o}ping University Electronic Press, Link{\"o}pings universitet}
}

@article{Revelle2009b,
 author = {Revelle, William and Zinbarg, Richard E.},
 year = {2009},
 title = {Coefficients Alpha, Beta, Omega, and the glb: Comments on Sijtsma},
 pages = {145--154},
 volume = {74},
 number = {1},
 issn = {0033-3123},
 journal = {Psychometrika},
 doi = {10.1007/s11336-008-9102-z}
}

@article{Sijtsma2009,
 abstract = {This discussion paper argues that both the use of Cronbach's alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score's reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test's internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals' test performance. The paper ends with a list of conclusions about the usefulness of alpha.},
 author = {Sijtsma, Klaas},
 year = {2009},
 title = {On the Use, the Misuse, and the Very Limited Usefulness of Cronbach's Alpha},
 pages = {107--120},
 volume = {74},
 number = {1},
 issn = {0033-3123},
 journal = {Psychometrika},
 doi = {10.1007/s11336-008-9101-0},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/20037639},
 file = {https://doi.org/10.1007/s11336-008-9101-0}
}

@article{McNeish2017,
 abstract = {Empirical studies in psychology commonly report Cronbach's alpha as a measure of internal consistency reliability despite the fact that many methodological studies have shown that Cronbach's alpha is riddled with problems stemming from unrealistic assumptions. In many circumstances, violating these assumptions yields estimates of reliability that are too small, making measures look less reliable than they actually are. Although methodological critiques of Cronbach's alpha are being cited with increasing frequency in empirical studies, in this tutorial we discuss how the trend is not necessarily improving methodology used in the literature. That is, many studies continue to use Cronbach's alpha without regard for its assumptions or merely cite methodological articles advising against its use to rationalize unfavorable Cronbach's alpha estimates. This tutorial first provides evidence that recommendations against Cronbach's alpha have not appreciably changed how empirical studies report reliability. Then, we summarize the drawbacks of Cronbach's alpha conceptually without relying on mathematical or simulation-based arguments so that these arguments are accessible to a broad audience. We continue by discussing several alternative measures that make less rigid assumptions which provide justifiably higher estimates of reliability compared to Cronbach's alpha. We conclude with empirical examples to illustrate advantages of alternative measures of reliability including omega total, Revelle's omega total, the greatest lower bound, and Coefficient H. A detailed software appendix is also provided to help researchers implement alternative methods. (PsycINFO Database Record},
 author = {McNeish, Daniel},
 year = {2017},
 title = {Thanks Coefficient Alpha, We'll Take It From Here},
 issn = {1082-989X},
 journal = {Psychological Methods},
 doi = {10.1037/met0000144},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/28557467}
}

@article{Savalei2019,
 author = {Savalei, Victoria and Reise, Steven P.},
 year = {2019},
 title = {Don't forget the model in your model-based reliability coefficients: A reply to McNeish (2018)},
 pages = {36},
 volume = {5},
 number = {1},
 issn = {2474-7394},
 journal = {Collabra: Psychology},
 doi = {10.1525/collabra.247}
}

@article{Widaman2022,
 abstract = {Measurement is fundamental to all research in psychology and should be accorded greater scrutiny than typically occurs. Among other claims, McNeish and Wolf (Thinking twice about sum scores. Behavior Research Methods, 52, 2287-2305) argued that use of sum scores (a) implies that a highly constrained latent variable model underlies items comprising a scale, and (b) may misrepresent or bias relations with other criteria. The central claim by McNeish and Wolf that use of sum scores requires the assumption that a parallel test model underlies item responses is incorrect and without psychometric merit. Instead, if a set of items is unidimensional, estimators of reliability are available even if the factor model underlying the set of items does not have a highly constrained form. Thus, dimensionality of a set of items is the key issue, and whether strict constraints on parameter estimates do or do not hold dictate the appropriate way to estimate reliability. McNeish and Wolf also claimed that more precise forms of scoring, such as estimating factor scores, would be preferable to sum scores. We provide analytic bases for reliability estimation and then provide several demonstrations of reliability estimation and the relative advantages of sum scores and factor scores. We contend that several claims by McNeish and Wolf are questionable and that, as a result, multiple recommendations they made and conclusions they drew are incorrect. The upshot is that, once the dimensional structure of a set of items is verified, sum scores often have a solid psychometric basis and therefore are frequently quite adequate for psychological research.},
 author = {Widaman, Keith F. and Revelle, William},
 year = {2022},
 title = {Thinking thrice about sum scores, and then some more about measurement and analysis},
 issn = {1554-3528},
 journal = {Behavior Research Methods},
 doi = {10.3758/s13428-022-01849-w},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/35469086}
}

@article{Cronbach1951,
 author = {Cronbach, Lee J.},
 year = {1951},
 title = {Coefficient alpha and the internal structure of tests},
 pages = {297--334},
 volume = {16},
 number = {3},
 issn = {0033-3123},
 journal = {Psychometrika},
 doi = {10.1007/BF02310555}
}

@book{Enders2010,
 author = {Enders, Craig K.},
 year = {2010},
 title = {Applied missing data analysis},
 keywords = {Data-analyse;Datenauswertung;Fehlende Daten;Forschung;Methodologie;Missing observations (Statistics);Ontbrekende gegevens;Social sciences;Sozialstatistik;Sozialwissenschaften;Statistik;Statistische methoden},
 address = {New York},
 publisher = {{Guilford Press}},
 isbn = {1606236393},
 series = {Methodology in the social sciences},
 file = {http://www.worldcat.org/oclc/456171131}
}

@article{Schafer2002,
 author = {Schafer, Joseph L. and Graham, John W.},
 year = {2002},
 title = {Missing data: Our view of the state of the art},
 pages = {147--177},
 volume = {7},
 number = {2},
 issn = {1082-989X},
 journal = {Psychological Methods},
 doi = {10.1037/1082-989X.7.2.147}
}

@article{Rose2019,
 author = {Rose, Norman and Wagner, Wolfgang and Mayer, Axel and Nagengast, Benjamin},
 year = {2019},
 title = {Model-Based Manifest and Latent Composite Scores in Structural Equation Models},
 pages = {243},
 volume = {5},
 number = {1},
 issn = {2474-7394},
 journal = {Collabra: Psychology},
 doi = {10.1525/collabra.143}
}

@article{McNeish2020b,
 abstract = {A common way to form scores from multiple-item scales is to sum responses of all items. Though sum scoring is often contrasted with factor analysis as a competing method, we review how factor analysis and sum scoring both fall under the larger umbrella of latent variable models, with sum scoring being a constrained version of a factor analysis. Despite similarities, reporting of psychometric properties for sum scored or factor analyzed scales are quite different. Further, if researchers use factor analysis to validate a scale but subsequently sum score the scale, this employs a model that differs from validation model. By framing sum scoring within a latent variable framework, our goal is to raise awareness that (a) sum scoring requires rather strict constraints, (b) imposing these constraints requires the same type of justification as any other latent variable model, and (c) sum scoring corresponds to a statistical model and is not a model-free arithmetic calculation. We discuss how unjustified sum scoring can have adverse effects on validity, reliability, and qualitative classification from sum score cut-offs. We also discuss considerations for how to use scale scores in subsequent analyses and how these choices can alter conclusions. The general goal is to encourage researchers to more critically evaluate how they obtain, justify, and use multiple-item scale scores.},
 author = {McNeish, Daniel and Wolf, Melissa Gordon},
 year = {2020},
 title = {Thinking twice about sum scores},
 pages = {2287--2305},
 volume = {52},
 number = {6},
 issn = {1554-3528},
 journal = {Behavior Research Methods},
 doi = {10.3758/s13428-020-01398-0},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32323277}
}

@Manual{RStudio2023,
    title = {RStudio: Integrated Development Environment for R},
    author = {{Posit team}},
    organization = {Posit Software, PBC},
    address = {Boston, MA},
    year = {2023},
    url = {http://www.posit.co/},
  }
  

@article{Enders2023,
 abstract = {The year 2022 is the 20th anniversary of Joseph Schafer and John Graham's paper titled {\textquotedbl}Missing data: Our view of the state of the art,{\textquotedbl} currently the most highly cited paper in the history of Psychological Methods. Much has changed since 2002, as missing data methodologies have continually evolved and improved; the range of applications that are possible with modern missing data techniques has increased dramatically, and software options are light years ahead of where they were. This article provides an update on the state of the art that catalogs important innovations from the past two decades of missing data research. The paper addresses topics described in the original paper, including developments related to missing data theory, full information maximum likelihood, Bayesian estimation, multiple imputation, and models for missing not at random processes. The paper also describes newer factored regression specifications and missing data handling for multilevel models, both of which have been a focus of recent research. The paper concludes with a summary of the current software landscape and a discussion of several practical issues. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
 author = {Enders, Craig K.},
 year = {2023},
 title = {Missing data: An update on the state of the art},
 issn = {1082-989X},
 journal = {Psychological Methods},
 doi = {10.1037/met0000563},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/36931827}
}